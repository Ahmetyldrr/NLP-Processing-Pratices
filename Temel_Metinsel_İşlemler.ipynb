{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MjpLlNs4SvyO",
        "dGO45bD1RHR6",
        "wg2YUe5eShP-",
        "fTflybFtHa-i"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0MyMV2o4t7DI9zTBLs6oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmetyldrr/NLP-Processing-Pratices/blob/main/Temel_Metinsel_%C4%B0%C5%9Flemler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bölüm 1: NLP'ye Giriş**"
      ],
      "metadata": {
        "id": "MjpLlNs4SvyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Heyecan verici Doğal Dil İşleme (NLP) dünyasına hoş geldiniz. Bu bölüm, NLP'nin temel kavramlarını ve temel unsurlarını anlamak için bir geçit görevi görür. Bu yolculuğa çıkarken, NLP'nin ne olduğunu, neden önemli olduğunu ve çeşitli alanlarda nasıl uygulandığını keşfedeceğiz. Bu bölümün sonunda, NLP'nin temel prensipleri hakkında sağlam bir anlayışa sahip olacak ve daha teknik yönlere derinlemesine dalmaya hazır olacaksınız.\n",
        "NLP, dilbilim, bilgisayar bilimi ve yapay zekayı harmanlayan büyüleyici bir alandır. Makinelerin insan dilini değerli bir şekilde yorumlamasını, anlamasını ve yanıtlamasını sağlar. Günümüzün veri odaklı dünyasında NLP, arama motorlarından ve çeviri hizmetlerinden sohbet robotlarına ve duygu analizi araçlarına kadar birçok uygulamanın kritik bir bileşeni haline gelmiştir.\n",
        "Bu bölüm temel bir soruyla başlıyor: Doğal Dil İşleme Nedir? NLP'nin tanımını, kapsamını ve uygulamalarını inceleyerek, takip edecek daha detaylı tartışmalar için sahneyi hazırlayan kapsamlı bir genel bakış sunacağız."
      ],
      "metadata": {
        "id": "Xp70bITzSs53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bölüm 2: Temel Metin İşlemleri**\n"
      ],
      "metadata": {
        "id": "dGO45bD1RHR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Metin Verilerini Anlamak**"
      ],
      "metadata": {
        "id": "wg2YUe5eShP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metin verileri doğası gereği yapılandırılmamıştır ve makaleler, sosyal medya gönderileri, e-postalar, sohbet mesajları, incelemeler ve daha fazlası gibi çeşitli biçimlerde olabilir. Yapılandırılmış yapısı nedeniyle makineler tarafından kolayca analiz edilebilen sayısal verilerin aksine, metin verileri yapılandırılmış bir biçime dönüştürmek için özel işleme ve işleme teknikleri gerektirir.\n",
        "Bu dönüşüm, algoritmaların metinde yer alan bilgileri verimli bir şekilde işleyebilmesi ve anlayabilmesi için olmazsa olmazdır. Nüansları, deyimleri ve çeşitli sözdizimleriyle insan dilinin karmaşıklığı, bu göreve ek bir zorluk katmanı ekler.\n",
        "Bu nedenle, metin verilerinden anlam çıkarmak ve anlamlı çıkarımlar yapmak için doğal dil işleme (NLP), makine öğrenmesi teknikleri ve çeşitli metin madenciliği stratejileri gibi gelişmiş yöntemler kullanılmaktadır.\n",
        "Bu yöntemler, mevcut metinsel bilgilere dayanarak eğilimleri kategorize etmeye, özetlemeye ve hatta tahmin etmeye yardımcı olur."
      ],
      "metadata": {
        "id": "cyYLV8JASYUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Metin Temizleme: Durdurma Sözcüğü Kaldırma, Köklendirme, Lemmatizasyon**"
      ],
      "metadata": {
        "id": "nQSOcClvSQzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metinlerin içinde bir çok karakter noktalama ve farklı durumlar vardır. Veri ön işleme aşamalarında temizleme yapılması gerekmektedir.\n",
        "Metin verilerinden anlam çıkarmak ve anlamlı içgörüler çıkarmak için kullanılır. Bu yöntemler, mevcut metinsel bilgilere dayanarak eğilimleri kategorize etmeye, özetlemeye ve tahmin etmeye yardımcı olur.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mpw55RgjRPYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metin verilerinin ön işlenmesi, herhangi bir Doğal Dil İşleme (NLP) kanalında kritik bir adımdır. Uygun ön işleme, metnin temiz, tutarlı ve makine öğrenimi modelleri tarafından kolayca analiz edilebilecek bir formatta olmasını sağlar. Bu adım, ham metin verilerini daha fazla analiz için hazırlamak üzere çeşitli teknikler ve yöntemler içerir. Metnin ön işlenmesinin temel nedenleri şunlardır"
      ],
      "metadata": {
        "id": "XYWkBEEzSW5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gürültü Azaltma**"
      ],
      "metadata": {
        "id": "yXc0BZcTSfJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Noktalama İşaretlerinin Kaldırılması"
      ],
      "metadata": {
        "id": "LFLRiojPSisA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Durdurma Sözcüklerinin Kaldırılması ( the , is , in, and )"
      ],
      "metadata": {
        "id": "tfuZ5hSZSm65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FeKcMreQWPaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Özel Öğrelerin Kaldırılması ( HTML , kodlar vs )"
      ],
      "metadata": {
        "id": "ZWyc-6ArSvfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, when text data is free from unnecessary noise, tokenization, stemming, and lemmatization processes become more efficient and accurate <br>\n",
        "Text gereksiz gürültüden kurtulduğunda işlemlerin doğruluk payı artmaktadır\n"
      ],
      "metadata": {
        "id": "bwKl5Oq9S-5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standartizasyon**"
      ],
      "metadata": {
        "id": "5cagsh8yTTRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Küçük harf"
      ],
      "metadata": {
        "id": "RsIOsPpbWLHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Köklendirme\n",
        "\n",
        "Mesela running kelimesini run olarak almak\n",
        "\n"
      ],
      "metadata": {
        "id": "sGZehL5pWNSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Lemmatizasyon\n",
        "\n",
        "Mesela \"daha iyi\" kelimesini \"iyi\" şeklinde alınmasıdır."
      ],
      "metadata": {
        "id": "UtcmBFA_WP-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Özellik Çıkarımı**"
      ],
      "metadata": {
        "id": "WT3-PocSWpxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ham metni özelliklere çıkarmak makine öğrenmesi açısından en önemli parçalardan bir tanesidir.\n",
        "Bu kalıpların çıkarılması metin verilerine dayalı sınıflandırmalar veya tahminler için kullanılır."
      ],
      "metadata": {
        "id": "d2ZQH5DEWtwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Tokenleştirme\n",
        "\n",
        "Verileri daha küçük parçalar halinde gösterme işlemidir."
      ],
      "metadata": {
        "id": "aqt1KrMVZ3Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Vektörleştirme\n",
        "\n",
        "Verileri makinenin anlayabileceği sayısal verilere dönüştürme işlemidir.\n",
        "\n",
        " Techniques such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word2Vec are commonly employed for this conversion.\n",
        "\n",
        " Bu dönüşüm için genellikle Kelime Torbası (BoW), Terim Frekansı-Ters Belge Frekansı (TF-IDF) ve Word2Vec gibi teknikler kullanılır. Bu sayısal gösterimler kritik öneme sahiptir çünkü makine öğrenimi algoritmalarının metin verileri üzerinde karmaşık matematiksel işlemler gerçekleştirmesini sağlayarak daha derin analiz ve içgörüler sağlar."
      ],
      "metadata": {
        "id": "F9qNVGmCaGi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Embedding\n",
        "\n",
        "Bu işlem kelimeleri ya da cümleleri daha yüksek boyutlu vektörlere eşlendiği durumu ifade eder.\n",
        "\n",
        "Word2Vec, GloVe ve BERT gibi popüler yöntemler bu gömmeleri oluşturmak için sıklıkla kullanılır.\n",
        "\n"
      ],
      "metadata": {
        "id": "6-Ld1rC7ab4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ham metni bu özelliklere dönüştürerek, makine öğrenimi modelleri verileri daha iyi anlayabilir ve yorumlayabilir. Bu işlem sırasında çıkarılan özellikler, algoritmaların metinden öğrenmesi için gerekli girdiyi sağlar ve bu sayede kalıpları tanımalarını, doğru tahminlerde bulunmalarını ve duygu analizi, metin sınıflandırması ve dil çevirisi gibi çeşitli NLP görevlerini gerçekleştirmelerini sağlar."
      ],
      "metadata": {
        "id": "aQCOXVv8bBRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ham Metin Verilerini Keşfetme**"
      ],
      "metadata": {
        "id": "xvE9vNZubJQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing (NLP) enables computers to understand human language.\""
      ],
      "metadata": {
        "id": "RMGeh9R7bPHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\\\\\\\nLength of the text:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU6M3xG9bRHQ",
        "outputId": "68597881-b478-4a3c-abe7-bdcd6a636a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\\\nLength of the text: 81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_characters = set(text)\n",
        "print(\"\\\\\\\\nUnique characters:\", unique_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZgpaWn9bUWi",
        "outputId": "4561beb6-00c5-4f53-bc7b-3ba1aa822688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\\\nUnique characters: {'u', 'P', 'd', ' ', 'h', 'L', 'm', 's', 't', 'o', 'b', 'r', 'p', '(', 'N', 'i', 'n', '.', 'e', 'a', 'l', ')', 'c', 'g'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "print(\"\\\\\\\\nNumber of words:\", len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4alcS9fbaWY",
        "outputId": "7217f3a5-3746-4df0-a7f8-5a791d4c9b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\\\nNumber of words: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metin Verileri ile ilgili Zorluklar**"
      ],
      "metadata": {
        "id": "UCb5HJiXbhoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Belirsizlik - Ambiguity\n",
        "\n",
        "Bazı kelimeler birden fazla anlama sahip olabilir.Dilin bu özelliği algoritmalar için anlam sorunu oluşturabilir.Bu nedenle bağlamsal bilgi olmadan algoritma bunu yanlış değerlendirebilir.\n"
      ],
      "metadata": {
        "id": "WDpZmx-gbmtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Değişkenlik -  Variability\n"
      ],
      "metadata": {
        "id": "xC7a7U7XcHnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bazen sosyal medya gönderimleri kısaltma ,argo vb özellikte olabilir.\n",
        "\n",
        "Ek olarak, metnin yazıldığı bağlam, yapısını ve anlamını etkileyebilir. Örneğin, \"bankayı kırmak\" gibi bir ifade, finansal bir bağlamda aşırı harcama anlamına gelebilir, ancak farklı bir bağlamda, bir bankaya girmenin fiziksel bir eylemini ifade edebilir. Bu bağlamsal nüansları anlamak, doğru metin analizi için esastır."
      ],
      "metadata": {
        "id": "0gFmYjHXcMBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Gürültülü Veri - Noisy Data"
      ],
      "metadata": {
        "id": "BQdvHih3cu9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "İnternetten aldığımız veriler örnek olarak HTML gibi etiketler içeriyor olabilir.\n",
        "\n",
        "Düzgün bir şekilde temizlenip filtrelenmezlerse, gürültülü veriler NLP modellerinin performansını önemli ölçüde engelleyebilir\n",
        "\n",
        "İlgisiz bilgilerin varlığı, modellerin sahte desenler ve korelasyonlar öğrenmesine yol açabilir ve böylece etkinliklerini ve doğruluklarını azaltabilir."
      ],
      "metadata": {
        "id": "rkKAO3FUc5eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 - Yüksek Boyutluluk -  High Dimensionality\n",
        "\n",
        "Yüksek boyut hesaplama karmaşıklığına neden olabilir.Aşırı uyum ve başka zorluklara neden olabilir.\n",
        "\n",
        "a - Hesaplama Karmaşıklığı - Computational Complexity\n",
        "\n",
        "b - Aşırı Uyum - Overfitting\n",
        "\n",
        "c - Boyutluluk Laneti - Curse of Dimensionality\n",
        "\n",
        "boyut sayısı arttıkça veri noktalarının seyrek hale gelmesidir. Bu seyreklik, algoritmaların verilerde anlamlı örüntüler ve ilişkiler bulmasını zorlaştırır. Ek olarak, veri noktaları arasındaki mesafe daha az bilgilendirici hale gelir ve kümeleme ve en yakın komşu araması gibi görevleri karmaşık hale getirir.\n",
        "\n",
        "d - Özellik Seçimi ve Mühendisliği - Feature Selection and Engineering\n",
        "\n",
        "Terim Frekansı-Ters Belge Frekansı (TF-IDF), Temel Bileşen Analizi (PCA) ve Word2Vec ve BERT gibi çeşitli yerleştirme yöntemleri gibi teknikler, boyutluluğu azaltmaya ve makine öğrenimi modellerinin performansını iyileştirmeye yardımcı olabilir.\n",
        "\n",
        "e -  Depolama ve Ölçeklenebilirlik - Storage and Scalability\n",
        "\n",
        "Yüksek boyutlu verileri depolamak ve yönetmek, özellikle büyük ölçekli metin korpuslarıyla uğraşırken zorlu olabilir\n",
        "\n",
        "Bununla başa çıkmak için bazı işlemler yapılabilir.\n",
        "\n",
        "a - Boyut Azaltma - Dimensionality Reduction\n",
        "\n",
        "Methods such as PCA, Singular Value Decomposition (SVD), and t-Distributed Stochastic Neighbor Embedding (t-SNE) can reduce the number of dimensions while preserving the most important information.\n",
        "\n",
        "PCA , SVD ve t-SNE gibi yöntemler kullanılabilir.\n",
        "\n",
        "b - Düzenleme -  Regularization\n",
        "\n",
        "L1 ve L2 düzenlemesi gibi teknikler, modeldeki büyük katsayılar için ceza ekleyerek aşırı uyumu önlemeye yardımcı olabilir.\n",
        "\n",
        "c - Gelişmiş Gömmeler - Advanced Embeddings\n",
        "\n",
        "Word2Vec, GloVe ve BERT gibi gelişmiş kelime gömme tekniklerini kullanmak, kelimeler arasındaki anlamsal ilişkileri yakalayabilir ve özellik alanının boyutluluğunu azaltabilir.\n",
        "\n",
        " **Duygu ve Öznellik - Sentiment and Subjectivity**\n",
        "\n",
        " Metinler genellikle duygular içerir. bu anlamda metnin pozitif mi , negatif mi , nötr mü olduğunu belirlemek önemlidir. Bazen kelimenin temel anlamı olumsuz olmasına rağmen temelde olumlu bir cümle yaratabilir. Mesela \"fena değil\"\n",
        "\n",
        " Alaycılık ve ironi genellikle ton, bağlam ve paylaşılan kültürel bilgiye dayanır.\n",
        "\n",
        " Bu zorlukların üstesinden gelmek için gelişmiş NLP teknikleri ve modelleri kullanılır. Belirteçleştirme, durdurma sözcüğü kaldırma, köklendirme ve lemmatizasyon gibi teknikler, metni ön işleme tabi tutmaya ve standartlaştırmaya yardımcı olarak analiz etmeyi kolaylaştırır. BERT ve GPT-3 gibi gelişmiş modeller, kelimeler arasındaki bağlamı ve bağımlılıkları anlamak ve duygu analizinin doğruluğunu artırmak için tasarlanmıştır.\n",
        "Metindeki duygu ve öznelliğin analizi, insan dilinin nüanslı ve çeşitli doğası nedeniyle karmaşık bir görevdir. Altta yatan duyguları doğru bir şekilde yakalamak için etkili ön işleme ve gelişmiş modelleme esastır.\n",
        "\n",
        "\n",
        " **Bağlam ve Bağımlılık -  Context and Dependency**\n",
        "\n",
        " Bir metnin anlamını anlamak genellikle bağlamı ve kelimeler arasındaki bağımlılıkları dikkate almayı gerektirir. Örneğin, \"fena değil\" ifadesini ele alalım. İlk bakışta, \"kötü\" kelimesi olumsuz bir duyguyu çağrıştırır. Ancak, \"değil\" ile eşleştirildiğinde, ifade aslında olumlu bir duyguyu iletir ve bir şeyin tatmin edici veya hatta iyi olduğunu gösterir. Bu örnek, tek tek kelimelerin bağlamlarına bağlı olarak nasıl farklı anlamlar taşıyabileceğini göstermektedir."
      ],
      "metadata": {
        "id": "-tSNEmoWiLed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu bağımlılıkları ve bağlamı yakalamak, doğru metin analizi için olmazsa olmazdır. Doğal dil işlemede (NLP), bu yalnızca kelimelerin kendilerini değil, bir cümle veya daha büyük bir metin gövdesi içinde birbirleriyle nasıl ilişkilendiklerini anlamak anlamına gelir."
      ],
      "metadata": {
        "id": "EVGu_WtX8jF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dil Çeşitliliği - Language Diversity**"
      ],
      "metadata": {
        "id": "xcXRkPFZ8ntV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dil çeşitliliği, her biri kendine özgü dil bilgisi kuralları, kelime bilgisi ve yazı sistemlerine sahip, dünya çapında çok sayıda dil ve lehçenin varlığına işaret eder. Bu çeşitlilik, Doğal Dil İşleme (NLP) alanında önemli bir zorluk teşkil eder. Odak noktasının tek bir dil olduğu tek dilli bir yaklaşımın aksine, birden fazla dili veya lehçeyi etkili bir şekilde işleyebilen NLP modelleri geliştirmek önemli miktarda çaba ve kaynak gerektirir."
      ],
      "metadata": {
        "id": "_w7Q9hrR8yMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alaycılık ve İroni - Sarcasm and Irony**"
      ],
      "metadata": {
        "id": "bvOLJ7279Aq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metindeki alaycılığı ve ironiyi tespit etmek bir diğer önemli zorluktur. Bu ifade biçimleri genellikle algoritmaların doğru bir şekilde yorumlamasının zor olduğu ton, bağlam ve kültürel bilgiye dayanır."
      ],
      "metadata": {
        "id": "PKqv1ZYZ9JU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temel Metin Ön İşleme Adımları**"
      ],
      "metadata": {
        "id": "3L6Kw5pT9Nb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing (NLP) enables computers to understand human language.\""
      ],
      "metadata": {
        "id": "LAJNufhYDbTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QhsBuP9XDipS",
        "outputId": "e31e2f4f-e768-4c09-fe85-515ec7461e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing (nlp) enables computers to understand human language.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UbzsBd9uDlZN",
        "outputId": "c7bbc0f0-7a74-4b93-ffb9-e528ae2989f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural Language Processing NLP enables computers to understand human language'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ49CuVbDu9R",
        "outputId": "c7002e5d-f2e7-47f7-c3ab-fee5bf3a9c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'NLP',\n",
              " 'enables',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.1 Stop Word Removal - Durdurma Sözcüğünün Kaldırılması**"
      ],
      "metadata": {
        "id": "_ZWZ4TQHD7Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durdurma sözcüklerinin kaldırılması  \"the\", \"is\", \"in\", \"and\" vb."
      ],
      "metadata": {
        "id": "t9uIKcv7EBGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Wpor73EOZV",
        "outputId": "86985cbe-4690-4b31-901e-51b8e5067cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing enables computers to understand human language.\""
      ],
      "metadata": {
        "id": "pPnBICm3Eb_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzmBm7EMEdni",
        "outputId": "7cf063ec-7840-4724-fff2-3d1ab0d8fed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '(NLP)',\n",
              " 'enables',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mS9WytSEhKv",
        "outputId": "b1ea63b3-caa7-4552-d745-d52f09662c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '(NLP)',\n",
              " 'enables',\n",
              " 'computers',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Burada \"to\" ifadesi atıldı."
      ],
      "metadata": {
        "id": "kZhcjD6jE2v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.2 Köklendirme - Stemming**"
      ],
      "metadata": {
        "id": "Fmiz4ju9E9Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Köklendirme Nasıl Çalışır\n",
        "\n",
        "Köklendirme, kelimelerden ekleri, önekleri veya diğer ekleri kaldırarak elde edilir. En yaygın kullanılan köklendirme algoritması, Martin Porter tarafından 1980'de geliştirilen Porter Stemmer'dır. Bu algoritma, kelimeleri köklerine dönüştürmek için bir dizi kural uygular."
      ],
      "metadata": {
        "id": "Xx6atg6jFD4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "text = \"Natural Language Processing enables computers to understand human language.\"\n",
        "tokens = text.split()\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSRqqUaiFIGE",
        "outputId": "6153712a-1492-4828-9d38-628112f236cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'enables',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "E8fkCgLxFRsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiLoP18RFUHU",
        "outputId": "8a044a11-3a3d-4fd2-a0f7-a99347350918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " 'enabl',\n",
              " 'comput',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bu işlemler diğer dillerde kullanımı uygun olmayabişlir ayrıca köklendirme işlemi çok agresif olduğunda bu soruna neden olabilir. Türk.e için bu konuda çok fazla ek vs olmasından kaynaklı bu tekniğin kullanılması sorunlara neden olur.Bu konuda yeni yöntemler vs denenmesi gerekmektedir."
      ],
      "metadata": {
        "id": "FzrX5YQHF4Rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.3 Lemmatizasyon - Lemmatizasyon**"
      ],
      "metadata": {
        "id": "ZoR8hVf5GKAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizasyon, doğal dil işlemede (NLP) sözcükleri lemma olarak bilinen temel veya kök biçimlerine dönüştüren önemli bir tekniktir. Genellikle önekleri veya sonekleri kesen köklendirmenin aksine, lemmatizasyon daha karmaşıktır ve bağlamı ve sözcük türünü dikkate alarak sözcükleri sözlük biçimlerine indirgemeyi içerir. Bu, lemmatizasyonu çeşitli NLP görevleri için daha doğru ve anlamlı hale getirir."
      ],
      "metadata": {
        "id": "SgVhUFRuGVb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizasyon Nasıl Çalışır\n",
        "\n",
        "Lemmatizasyon, kelimelerin temel biçimini döndürmek için bir sözlük ve morfolojik analiz kullanmayı içerir. İşlem genellikle doğru olması için kelimenin sözcük türünün bilinmesini gerektirir. Örneğin, \"saw\" kelimesi bir isim veya fiil olabilir ve lemmatizasyon bu kullanımlar arasında ayrım yapabilir."
      ],
      "metadata": {
        "id": "ykV9xL2DGakP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "yYgbHFPCGeWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1yBCyi2GhRk",
        "outputId": "008a7bee-55be-43b2-a2f6-94a35f880e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing enables computers to understand human language.\""
      ],
      "metadata": {
        "id": "SBQrVEu0Gi5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "LuMAGjuPGkOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "WTktcW6gGl4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtSjwDTDGnXu",
        "outputId": "a1ff67ef-5c83-4da1-c4bc-4cf92a6f7dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'enables',\n",
              " 'computer',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'language.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\\\\\\\nLemmatized Tokens:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSo56QqBGx1p",
        "outputId": "8faedae1-b592-4b31-e926-ace3bfe69cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language.']\n",
            "\\\\nLemmatized Tokens:\n",
            "['Natural', 'Language', 'Processing', 'enables', 'computer', 'to', 'understand', 'human', 'language.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Regular Expressions**"
      ],
      "metadata": {
        "id": "fTflybFtHa-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.1 Basics of Regular Expressions**"
      ],
      "metadata": {
        "id": "CGq8dFQmHj8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genellikle regex olarak kısaltılan düzenli ifade, metin içindeki karakter dizilerini eşleştirmek için kullanılan bir arama modelini tanımlayan bir karakter dizisidir. Bu güçlü araç, metnin bölümlerini bulmak, çıkarmak veya değiştirmek için kullanılabilen belirli modeller tanımlayarak karmaşık metin arama ve düzenlemesine olanak tanır."
      ],
      "metadata": {
        "id": "pckx4EDBH13x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "6mgnPaPuH6qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n"
      ],
      "metadata": {
        "id": "vjJGTmpOH8J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'fox'\n",
        "pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7gQJwLJXIBpF",
        "outputId": "fc557169-d1a0-49b4-a121-15294de27b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fox'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match1 = re.search(pattern, text)\n",
        "match1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcdaOYAQIIFy",
        "outputId": "bce03a18-567b-474e-afa3-f27073d6d487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(16, 19), match='fox'>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if match1:\n",
        "    print(\"Match found:\", match1.group())\n",
        "else:\n",
        "    print(\"No match found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijod0uswIR1d",
        "outputId": "7eed8018-b379-42cc-b66c-d3bb48d56f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match found: fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.2 Yaygın Regex Desenleri ve Sözdizimi**"
      ],
      "metadata": {
        "id": "0Xuy1Sb-J65c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nokta (.)**"
      ],
      "metadata": {
        "id": "qrAWSn0MMGSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Yeni satır karakteri hariç, herhangi bir tek karakterle eşleşir."
      ],
      "metadata": {
        "id": "0B60kO56MKKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'c.t'  # 'c' ile başlayıp, herhangi bir karakter ve 't' ile biten diziler\n",
        "text = \"catar cot cut bit\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['cat', 'cot', 'cut']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze98efshMH-9",
        "outputId": "57ff1131-bf59-4ca4-db70-7cb8be6d57ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'cot', 'cut']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Şapka (^)**"
      ],
      "metadata": {
        "id": "f-aq-xbZMwhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Desenin, metnin başında yer aldığını belirtir."
      ],
      "metadata": {
        "id": "lURyqNA6Mzs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'^Hello'\n",
        "text = \"Hello world, Hello Python\"\n",
        "match = re.search(pattern, text)\n",
        "if match:\n",
        "    print(\"Başlangıçta 'Hello' var.\")  # Çıktı: Başlangıçta 'Hello' var."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia_4G3mAMyG5",
        "outputId": "c27dd9fd-7d94-40a8-c302-1b43e3ff78c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Başlangıçta 'Hello' var.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Dolar İşareti ($)**"
      ],
      "metadata": {
        "id": "lRSGwVCwM6lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Desenin, metnin sonunda yer aldığını belirtir."
      ],
      "metadata": {
        "id": "o_1qAGetM7aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'Python$'\n",
        "text = \"Welcome to Python\"\n",
        "match = re.search(pattern, text)\n",
        "if match:\n",
        "    print(\"Metin 'Python' ile bitiyor.\")  # Çıktı: Metin 'Python' ile bitiyor."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "empyoyqXM-sP",
        "outputId": "f8759b3b-292d-4db4-cec6-9c38a021f44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metin 'Python' ile bitiyor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **4. Yıldız (*)**"
      ],
      "metadata": {
        "id": "oDaOJ5ulNHJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Önceki karakterin sıfır veya daha fazla tekrarını eşleştirir."
      ],
      "metadata": {
        "id": "hYNQGYXENMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'ab*'  # 'a' ile başlayıp, ardından sıfır veya daha fazla 'b'\n",
        "text = \"a ab abb abbb kkkbb abb baaabb\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['a', 'ab', 'abb', 'abbb']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVPukB68NOXC",
        "outputId": "b1a0fcc8-ddd8-4b74-db0f-a0fa640c53f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'ab', 'abb', 'abbb', 'abb', 'a', 'a', 'abb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Artı (+)**"
      ],
      "metadata": {
        "id": "8F_uYOPLNkuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Önceki karakterin bir veya daha fazla tekrarını eşleştirir."
      ],
      "metadata": {
        "id": "stz-fe79NnFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'ab+'  # 'a' ile başlayıp, ardından en az bir 'b'\n",
        "text = \"a ab abb abbb aabb aaa\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['ab', 'abb', 'abbb']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3gjm617NolR",
        "outputId": "f882e247-5401-4183-f365-ab2db7e99b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ab', 'abb', 'abbb', 'abb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Soru İşareti (?)**"
      ],
      "metadata": {
        "id": "TCr8BUTdNySn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Önceki karakterin sıfır veya bir tekrarını eşleştirir; yani karakter opsiyoneldir."
      ],
      "metadata": {
        "id": "COp8T7nGN0s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'ab?'  # 'a' ile başlayıp, ardından sıfır veya bir 'b'\n",
        "text = \"a ab abb baab\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['a', 'ab', 'ab']  # İlk 'abb' içinde yalnızca 'ab' eşleşir çünkü 'b' sadece bir kez kabul edilir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srtofM7LN2J6",
        "outputId": "93bbb5f9-6ab0-44fc-9e36-b3fe3e3ec1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'ab', 'ab', 'a', 'ab']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Köşeli Parantezler ([])**"
      ],
      "metadata": {
        "id": "NZXcyoClOACK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Köşeli parantezler içerisinde belirtilen karakter setlerinden herhangi biriyle eşleşir."
      ],
      "metadata": {
        "id": "fAa0w9fqOCOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'[abc]'  # a, b veya c karakterlerinden herhangi biri\n",
        "text = \"def cab babad\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['c', 'a', 'b']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GZfgrGmODj9",
        "outputId": "e4dfcd66-4cf9-4ae4-e72e-27485ac95fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c', 'a', 'b', 'b', 'a', 'b', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. \\d**"
      ],
      "metadata": {
        "id": "xb32FCehOL7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Bir basamak (digit) ile eşleşir; [0-9] aralığındaki herhangi bir rakam."
      ],
      "metadata": {
        "id": "Gq3Omtg8OObG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'\\d+'  # Bir veya daha fazla rakam\n",
        "text = \"There are 12 apples and 30 oranges. 221 33\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['12', '30']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5ywHM4OQYw",
        "outputId": "91259e46-abe6-4278-e71d-94e95c817ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12', '30', '221', '33']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. \\w**"
      ],
      "metadata": {
        "id": "1K66-MEGOZKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Herhangi bir alfasayısal karakter (harf, rakam, alt çizgi) ile eşleşir; [a-zA-Z0-9_] ile aynı anlama gelir."
      ],
      "metadata": {
        "id": "YTd3_yGjObsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'\\w+'  # Bir veya daha fazla alfasayısal karakter\n",
        "text = \"Hello_world 123! smile_x world , a , __,.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['Hello_world', '123']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waAc1eVCOeLK",
        "outputId": "659c41cf-17ee-4ed4-b44e-70d08826d6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello_world', '123', 'smile_x', 'world', 'a', '__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. \\s**"
      ],
      "metadata": {
        "id": "18d3LdC-O1Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Bir boşluk karakteri ile eşleşir; boşluk, sekme, yeni satır gibi karakterleri kapsar."
      ],
      "metadata": {
        "id": "DO0CLYERO3P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'\\s+'  # Bir veya daha fazla boşluk karakteri\n",
        "text = \"Hello   world\\tPython //m\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['   ', '\\t']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyxVgObKO6OE",
        "outputId": "c5aab639-3bd4-4c62-a246-a0eb284a8e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['   ', '\\t', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Dikey Çizgi (|)**"
      ],
      "metadata": {
        "id": "MgesGk3zPCkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama: Alternatif (veya) operatörüdür; bir desen veya diğer desenle eşleşir."
      ],
      "metadata": {
        "id": "gf7U6ECTPFAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'cat|dog'  # \"cat\" veya \"dog\" kelimelerinden herhangi biri\n",
        "text = \"I have a cat and a dog. cat and dogs\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)  # Çıktı: ['cat', 'dog']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGybVxCcPGWN",
        "outputId": "01d38f75-4736-45a5-9970-9e5047b0316c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'dog', 'cat', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Parantezler (())**"
      ],
      "metadata": {
        "id": "IyS3YzoKPL40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'(\\d+)-(\\d+)-(\\d+)'  # İki grup halinde rakamları yakalar, aralarında tire bulunan desen\n",
        "text = \"The serial number is 1232342-234-456344.\"\n",
        "match = re.search(pattern, text)\n",
        "if match:\n",
        "    print(\"Birinci grup:\", match.group(1))  # Çıktı: Birinci grup: 123\n",
        "    print(\"İkinci grup:\", match.group(2))   # Çıktı: İkinci grup: 456\n",
        "    print(\"üçüncü grup:\", match.group(3))   # Çıktı: İkinci grup: 456\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLngXy6lPOir",
        "outputId": "c69fa490-41a4-4b44-d7a0-fb7c05600a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birinci grup: 1232342\n",
            "İkinci grup: 234\n",
            "üçüncü grup: 456344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mail Adreslerini ayıkla**"
      ],
      "metadata": {
        "id": "Wl0XTf1tQSbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özetle, bu desen e-posta adreslerinde genellikle görülen formatı yakalamayı amaçlar:\n",
        "\n",
        "[geçerli karakterler]@[geçerli karakterler].[geçerli uzantı]"
      ],
      "metadata": {
        "id": "13BCk4mCQVxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Sample text with email addresses\n",
        "text = \"Please contact us at support@example.com or sales@example.com for further information.\"\n",
        "# Correct regex pattern to match email addresses\n",
        "pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
        "# Use re.findall() to find all matches\n",
        "emails = re.findall(pattern, text)\n",
        "# Display the extracted email addresses\n",
        "print(\"Extracted Email Addresses:\")\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPfV4wGRPtho",
        "outputId": "00e81d99-dea1-4b86-8595-06c38b89b258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "['support@example.com', 'sales@example.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Telefon numaralarını ayıkla**"
      ],
      "metadata": {
        "id": "saZxcIP9Qjy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenin Açıklaması:\n",
        "\n",
        "\\( ve \\): Parantez karakterlerini (yani \"(\" ve \")\") aramak için kullanılır.\n",
        "\n",
        "\\d{3}: Üç basamaktan oluşan bir sayı dizisini temsil eder.\n",
        "\n",
        "(boşluk): Parantez kapandıktan sonra gelen boşluk karakterini eşleştirir.\n",
        "\n",
        "\\d{3}: Üç basamaklı ikinci sayı dizisini eşleştirir.\n",
        "\n",
        "-: Tire (-) karakterini eşleştirir.\n",
        "\n",
        "\\d{4}: Son dört basamaklı sayı dizisini eşleştirir.\n",
        "\n",
        "Bu şekilde kodu çalıştırdığınızda, beklenen telefon numarası listesini almanız gerekir."
      ],
      "metadata": {
        "id": "W-EQh__EQ0LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Sample text with phone numbers\n",
        "text = \"Contact us at (123) 456-7890 or (987) 654-3210.\"\n",
        "# Correct regex pattern to match phone numbers\n",
        "pattern = r\"\\(\\d{3}\\) \\d{3}-\\d{4}\"\n",
        "# Use re.findall() to find all matches\n",
        "phone_numbers = re.findall(pattern, text)\n",
        "# Display the extracted phone numbers\n",
        "print(\"Extracted Phone Numbers:\")\n",
        "print(phone_numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW2L8-2BQopL",
        "outputId": "6e93a762-e638-4f43-fafe-8bcebd9d669a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Phone Numbers:\n",
            "['(123) 456-7890', '(987) 654-3210']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alt dizeleri değiştirme**"
      ],
      "metadata": {
        "id": "ghvR1E9iQ_10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Sample text\n",
        "text = \"The quick brown FOX jumps over the lazy dog. The fox is clever.\"\n",
        "# Define a pattern to match the word \"fox\"\n",
        "pattern = r\"FOX\"\n",
        "# Use re.sub() to replace \"fox\" with \"cat\"\n",
        "new_text = re.sub(pattern, \"XXX\", text)\n",
        "# Display the modified text\n",
        "print(\"Modified Text:\")\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTVYg8SRE7s",
        "outputId": "d2f4f178-920b-43af-c8fd-3aa76edb8d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified Text:\n",
            "The quick brown XXX jumps over the lazy dog. The fox is clever.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.4 Gelişmiş Regex Teknikleri**"
      ],
      "metadata": {
        "id": "zjKZiKEoRWYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarihleri ​​Çıkarma**"
      ],
      "metadata": {
        "id": "MKZzYMyfRaF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Sample text with dates\n",
        "text = \"The event is scheduled for 2022-08-15. Another event is on 15/08/2022.\"\n",
        "# Correct regex pattern to match dates\n",
        "pattern = r\"\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4})\\b\"\n",
        "# Use re.findall() to find all matches\n",
        "dates = re.findall(pattern, text)\n",
        "# Display the extracted dates\n",
        "print(\"Extracted Dates:\")\n",
        "print(dates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7OPk-5-RdsQ",
        "outputId": "13d2b096-d4ec-4388-e6fa-cac8ceffd1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Dates:\n",
            "['2022-08-15', '15/08/2022']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenin Açıklaması:\n",
        "\n",
        "\\b\n",
        "\n",
        "Kelime sınırını belirtir; tarih ifadesinin tam bir kelime olarak eşleşmesini sağlar.\n",
        "\n",
        "(?: ... )\n",
        "\n",
        "Non-capturing group: Parantez içindeki desen gruplamasını yapar, ancak yakalanan grup sonuçlara dahil edilmez.\n",
        "\n",
        "\\d{4}-\\d{2}-\\d{2}\n",
        "\n",
        "YYYY-AA-GG formatındaki tarihi yakalar.\n",
        "\n",
        "\\d{4}: 4 basamaklı yıl\n",
        "\n",
        "-: Tire karakteri\n",
        "\n",
        "\\d{2}: 2 basamaklı ay\n",
        "\n",
        "-: Tire karakteri\n",
        "\n",
        "\\d{2}: 2 basamaklı gün\n",
        "\n",
        "\\d{2}/\\d{2}/\\d{4}\n",
        "\n",
        "DD/AA/YYYY formatındaki tarihi yakalar.\n",
        "\n",
        "\n",
        "\\d{2}: 2 basamaklı gün\n",
        "\n",
        "/: Slash karakteri\n",
        "\n",
        "\\d{2}: 2 basamaklı ay\n",
        "\n",
        "/: Slash karakteri\n",
        "\n",
        "\\d{4}: 4 basamaklı yıl\n",
        "|\n",
        "Alternatif operatör; iki farklı tarih formatından birine eşleşmeyi sağlar.\n",
        "\n",
        "Bu şekilde kodu çalıştırdığınızda, metindeki her iki tarih formatı da doğru şekilde yakalanacaktır."
      ],
      "metadata": {
        "id": "nevNlAzvRr62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Tokenleştirme"
      ],
      "metadata": {
        "id": "mtVaDX_sSDmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetonlama, Doğal Dil İşleme (NLP) için metin ön işleme hattında temel bir adımdır. Bir metin parçasını jeton adı verilen daha küçük birimlere ayırmayı içerir. Bu jetonlar, söz konusu görevin belirli gereksinimlerine bağlı olarak kelimeler, cümleler veya hatta tek tek karakterler olabilir. Jetonlama, yapılandırılmamış metni, algoritmalar tarafından kolayca analiz edilebilen ve işlenebilen yapılandırılmış bir biçime dönüştürdüğü için önemlidir."
      ],
      "metadata": {
        "id": "VafRkBKBT0Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.3 Word Tokenization**"
      ],
      "metadata": {
        "id": "4FuPNBQ6YMp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Doyc9pB2UBaY",
        "outputId": "109f5a21-9c58-49b8-ab6b-660537d21a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural Language Processing enables computers to understand human language.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8zTihXdVsAR",
        "outputId": "24fa62aa-292b-48ef-dde0-b8a83c15064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy ile**"
      ],
      "metadata": {
        "id": "FjFT96ZVYF1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Sample text\n",
        "text = \"Natural Language Processing enables computers to understand human language.\"\n",
        "# Perform word tokenization\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Word Tokens:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFGzYQcrYWNg",
        "outputId": "dc579686-4869-4345-8ca0-35c53c241325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kelime Simgeleştirmenin Uygulamaları**"
      ],
      "metadata": {
        "id": "Gv_D3LtOYkZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metin Sınıflandırması :** Bu, metni önceden tanımlanmış kategorilere ayırmayı içerir ve spam tespiti, konu etiketleme ve daha iyi erişim ve yönetim için içeriği düzenleme gibi çeşitli uygulamalarda yararlı olabilir.\n",
        "\n",
        "**Duygu Analizi :** Bu uygulama, bir metinde ifade edilen duygunun olumlu, olumsuz veya nötr olup olmadığını belirlemeyi içerir. Müşteri geri bildirim analizinde, sosyal medya izlemede ve kamuoyunun fikrini ve duygusunu ölçmek için pazar araştırmasında yaygın olarak kullanılır.\n",
        "\n",
        "**Adlandırılmış Varlık Tanıma (NER) :** Bu teknik, bir metindeki varlıkları, kişilerin, kuruluşların, yerlerin, tarihlerin ve diğer önemli varlıkların adları gibi önceden tanımlanmış kategorilere tanımlamak ve sınıflandırmak için kullanılır. NER, bilgi çıkarma, içerik kategorizasyonu ve belgelerin aranabilirliğini artırmak için çok önemlidir.\n",
        "\n",
        "**Makine Çevirisi :** Bu, dil engellerini aşmak ve diller arası iletişimi sağlamak için gerekli olan metni bir dilden diğerine çevirmeyi içerir. Çok dilli içerik oluşturma, belgeleri çevirme ve farklı dillerde gerçek zamanlı iletişimi kolaylaştırmada uygulamaları vardır.\n",
        "\n",
        "**Bilgi Alma :** Bu uygulama, kullanıcı sorgularına dayalı olarak büyük veri kümelerinden ilgili bilgileri bulmaya odaklanır. Arama motorlarının, dijital kütüphanelerin ve büyük miktardaki metin verilerinden bilgilerin verimli bir şekilde alınmasını gerektiren diğer sistemlerin omurgasıdır."
      ],
      "metadata": {
        "id": "gRAecDGiYn_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.4 Cümle Tokenizasyonu**"
      ],
      "metadata": {
        "id": "l4yEIXOlY1eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "# Sample text\n",
        "text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n",
        "# Perform sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\")\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSL4Qqt0Y8jL",
        "outputId": "44710363-f51a-4709-dedc-de8343e4d2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "['Natural Language Processing enables computers to understand human language.', 'It is a fascinating field.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy ile**"
      ],
      "metadata": {
        "id": "53edNBoxZBjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Sample text\n",
        "text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n",
        "# Perform sentence tokenization\n",
        "doc = nlp(text)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(\"Sentences:\")\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5U4p79nZEIu",
        "outputId": "7c2ef550-472c-475d-adaa-a3753563a19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "['Natural Language Processing enables computers to understand human language.', 'It is a fascinating field.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4.5 Karakter Tokenizasyonu**"
      ],
      "metadata": {
        "id": "EKQoF0seZI9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing\"\n",
        "# Perform character tokenization\n",
        "characters = list(text)\n",
        "print(\"Characters:\")\n",
        "print(characters)"
      ],
      "metadata": {
        "id": "qXrPSjxjZPaY",
        "outputId": "58049393-2e8c-4c62-aa09-75aeecbecea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters:\n",
            "['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bölüm 3: NLP için Özellik Mühendisliği**"
      ],
      "metadata": {
        "id": "g1scQ3rzoOYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Kelime Torbası  - Bag of Words**"
      ],
      "metadata": {
        "id": "cS8173N_oYcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Bag of Words\" (BoW), doğal dil işlemede (NLP) metin gösterimi için kullanılan temel bir yöntemdir. Her bir belgeyi düzensiz bir kelime koleksiyonu olarak ele alarak, dilbilgisi, kelime sırası ve bağlamı göz ardı ederek ancak her bir kelimenin sıklığını koruyarak metni sayısal özelliklere dönüştürür."
      ],
      "metadata": {
        "id": "QZ8RV0Tkokde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.1 Kelime Torbası Modelini Anlamak**"
      ],
      "metadata": {
        "id": "hEZAXPH4orlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metni Simgeleştirme - Metin Tokenizasyon**"
      ],
      "metadata": {
        "id": "bZ8sHmHHow2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metni belirteçleme, metni ayrı ayrı kelimelere veya belirteçlere ayırma sürecini ifade eder. Bu, metin işleme ve analizinde ilk ve önemli adımdır. Belirteçleme, bir cümleyi, paragrafı veya tüm belgeyi onu oluşturan kelimelere veya alt kelimelere ayırmayı içerir. Örneğin, \"Doğal dil işleme eğlencelidir\" cümlesi, [\"Doğal\", \"dil\", \"işleme\", \"olmak\", \"eğlenceli\"] gibi bir kelime listesine belirteçlenir.\n",
        "Metni belirteçlere dönüştürerek, metin sınıflandırması, duygu analizi ve makine çevirisi gibi çeşitli doğal dil işleme (NLP) görevleri için verileri daha kolay analiz edebilir ve işleyebiliriz. Belirteçleştirme, kelime dağarcığının temelini oluşturacak sözcükleri ve model oluşturmadaki sonraki adımları belirlemeye yardımcı olur."
      ],
      "metadata": {
        "id": "gtQprj5IpCok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.2 Python'da Kelime BOW Uygulama**"
      ],
      "metadata": {
        "id": "eiXPtJgZpZbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sample text corpus\n",
        "documents = [\n",
        "\"Natural language processing is fun\",\n",
        "\"Language models are important in NLP \"\n",
        "]\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer on the text data\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Convert the result to an array\n",
        "bow_array = X.toarray()\n",
        "# Get the feature names (vocabulary)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\")\n",
        "print(vocab)\n",
        "print(\"\\\\\\\\nBag of Words Array:\")\n",
        "print(bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBgZBoaKpmWr",
        "outputId": "42fcd814-3306-4f02-f04f-e15ea692a113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['are' 'fun' 'important' 'in' 'is' 'language' 'models' 'natural' 'nlp'\n",
            " 'processing']\n",
            "\\\\nBag of Words Array:\n",
            "[[0 1 0 0 1 1 0 1 0 1]\n",
            " [1 0 1 1 0 1 1 0 1 0]]\n"
          ]
        }
      ]
    }
  ]
}